{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced21a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# 03_WRI_PerCountry_Metrics\n",
    "# --------------------------------------------------------------\n",
    "# Purpose\n",
    "#   For each country where WRI Urban Land Use rasters exist:\n",
    "#     1) Sample informal probabilities and classes per CSD block.\n",
    "#     2) Compare WRI-based \"deprived\" labels to RF-based rf_label\n",
    "#        across thresholds τ ∈ {0.1, 0.2, 0.3}.\n",
    "#     3) Save per-block metrics, country sweeps, and per-city counts.\n",
    "#\n",
    "# Inputs (NOT shipped in repo; user must prepare):\n",
    "#   - GPKG_ROOT: CSD segment files, one per country:\n",
    "#       {country}_rf_preds.gpkg   (contains rf_label, UC_NM_MN, etc.)\n",
    "#   - WRI_ROOT: city WRI rasters organised by country, e.g.:\n",
    "#       WRI_urban_landuse_v1/PerCountry_Files/<country>/*.tif\n",
    "#\n",
    "# Outputs (SHIPPED in repo/Zenodo):\n",
    "#   WRI/Outputs/PerCountry_Outputs/<country>/:\n",
    "#       - {country}_wri_informal_per_block.csv\n",
    "#       - {country}_wri_overlap_audit.csv\n",
    "#       - {country}_wri_vs_rf_threshold_sweep_country.csv\n",
    "#       - {country}_wri_vs_rf_per_block_with_preds.csv\n",
    "#       - {country}_overall_deprived_counts_by_tau.csv\n",
    "#       - {country}_per_city_deprived_counts_by_tau.csv\n",
    "#\n",
    "#   And a global report:\n",
    "#       WRI/Outputs/wri_country_processing_report.csv\n",
    "#\n",
    "# Notes\n",
    "#   - WRI rasters are downloaded from GEE (01_WRI_DataDownload.js),\n",
    "#     then manually arranged into PerCountry_Files/<country>/.\n",
    "#   - These rasters are not included in the repository due to size.\n",
    "# ==============================================================\n",
    "\n",
    "# ==============================================================\n",
    "# NOTE ON INFORMALITY METRICS\n",
    "# --------------------------------------------------------------\n",
    "# This script computes two WRI-based indicators:\n",
    "#   1) p_informal — share of pixels labeled {2,3} in the categorical LULC band.\n",
    "#   2) m_informal — average probability from the \"atomistic\" and\n",
    "#                   \"informal_subdivision\" bands (0–1).\n",
    "#\n",
    "# For FULL reproducibility, both are calculated.\n",
    "#\n",
    "# HOWEVER: The for further downstream tasks uses ONLY p_informal for all WRI–CSD\n",
    "# comparisons, figures, and quantitative results. m_informal is included\n",
    "# only as a secondary diagnostic and is not used in any analysis.\n",
    "# ==============================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e0fb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rasterio as rio\n",
    "from rasterio import mask as rio_mask\n",
    "from shapely.geometry import box\n",
    "from joblib import Parallel, delayed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dd056a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# 1️⃣ CONFIG (EDIT PATHS FOR YOUR ENVIRONMENT)\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "# CSD segment GPKGs (with rf_label)\n",
    "GPKG_ROOT = Path(\"../2_modelling/02_application/Filtered_80pct_allattributes\")\n",
    "\n",
    "\n",
    "# WRI city rasters organised by country (NOT in repo)\n",
    "#   Example structure:\n",
    "#   WRI_urban_landuse_v1/\n",
    "#       PerCountry_Files/\n",
    "#           india/\n",
    "#               mumbai_y2020.tif\n",
    "#               ...\n",
    "#           kenya/\n",
    "#               ...\n",
    "WRI_ROOT = Path(\".../WRI/PerCountry_Files\")\n",
    "\n",
    "\n",
    "# Output parent (this is what you expose in GitHub/Zenodo)\n",
    "OUT_PARENT = Path(\".../WRI/Outputs\")\n",
    "\n",
    "OUT_PARENT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CITY_COL = \"UC_NM_MN\"\n",
    "RF_COL   = \"rf_label\"\n",
    "\n",
    "TAUS = [0.1, 0.2, 0.3]\n",
    "MIN_VALID_PX = 1\n",
    "PROB_DIVISOR = 100.0\n",
    "POSSIBLE_ID_COLS = [\"block_id\", \"ID_SEG\", \"ID_HDC_G0\", \"ID_HDC_G0_SEG\", \"ID\"]\n",
    "\n",
    "# Keep GDAL single-threaded inside each process (more stable).\n",
    "os.environ.setdefault(\"GDAL_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"RIO_MAX_WORKERS\", \"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dd9077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# 2️⃣ HELPERS\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "def list_wri_countries(wri_root: Path) -> List[str]:\n",
    "    \"\"\"Return sorted list of country folder names under WRI_ROOT.\"\"\"\n",
    "    return sorted([p.name for p in wri_root.iterdir() if p.is_dir()])\n",
    "\n",
    "\n",
    "def list_wri_tifs(country_dir: Path) -> List[Path]:\n",
    "    \"\"\"List all GeoTIFFs in a given country folder.\"\"\"\n",
    "    return sorted(list(country_dir.glob(\"*.tif\")) + list(country_dir.glob(\"*.tiff\")))\n",
    "\n",
    "\n",
    "def read_blocks(gpkg_path: Path, city_col: str, rf_col: str) -> gpd.GeoDataFrame:\n",
    "    \"\"\"Read block GPKG, enforce presence of city and rf_label, clean geometry.\"\"\"\n",
    "    gdf = gpd.read_file(gpkg_path)\n",
    "    if city_col not in gdf.columns:\n",
    "        raise ValueError(f\"Missing '{city_col}' in {gpkg_path}\")\n",
    "    if rf_col not in gdf.columns:\n",
    "        raise ValueError(f\"Missing '{rf_col}' in {gpkg_path}\")\n",
    "\n",
    "    gdf = gdf[~gdf.geometry.is_empty & gdf.geometry.notna()].copy()\n",
    "    gdf[\"geometry\"] = gdf.geometry.buffer(0)\n",
    "    if gdf.crs is None:\n",
    "        raise ValueError(f\"{gpkg_path} has no CRS; please define it.\")\n",
    "    return gdf\n",
    "\n",
    "\n",
    "def read_raster_meta(tif: Path):\n",
    "    \"\"\"Return basic metadata for a WRI raster.\"\"\"\n",
    "    with rio.open(tif) as ds:\n",
    "        return dict(\n",
    "            crs=ds.crs,\n",
    "            bounds=box(*ds.bounds),\n",
    "            descriptions=list(ds.descriptions) if ds.descriptions else [],\n",
    "        )\n",
    "\n",
    "\n",
    "def band_map_from_descriptions(desc: List[str]) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Map band descriptions to indices:\n",
    "      - lulc\n",
    "      - atomistic (probability)\n",
    "      - informal_subdivision (probability)\n",
    "    \"\"\"\n",
    "    low = [d.lower() if d else \"\" for d in desc]\n",
    "    find = lambda key: (low.index(key) + 1) if key in low else None\n",
    "    bm = {\n",
    "        \"lulc\": find(\"lulc\"),\n",
    "        \"atomistic\": find(\"atomistic\"),\n",
    "        \"informal_subdivision\": find(\"informal_subdivision\"),\n",
    "    }\n",
    "    # fallback if lulc band not tagged\n",
    "    if bm[\"lulc\"] is None:\n",
    "        bm[\"lulc\"] = 1\n",
    "    return bm\n",
    "\n",
    "\n",
    "def block_stats_for_tif(block_row, tif_path: Path, bm: Dict[str, int]) -> Dict:\n",
    "    \"\"\"\n",
    "    For one block geometry and one WRI raster:\n",
    "      - valid_px: number of overlapping pixels with all bands\n",
    "      - p_informal: share of lulc pixels in {2,3}\n",
    "      - m_informal: average of atomistic + informal_subdivision\n",
    "                    probabilities (scaled 0–1)\n",
    "    \"\"\"\n",
    "    geom = block_row.geometry\n",
    "    out = {\"valid_px\": 0, \"p_informal\": np.nan, \"m_informal\": np.nan}\n",
    "\n",
    "    try:\n",
    "        with rio.open(tif_path) as ds:\n",
    "            r_bounds = box(*ds.bounds)\n",
    "            # Quick reject by geometry bbox\n",
    "            if not box(*geom.bounds).intersects(r_bounds):\n",
    "                return out\n",
    "\n",
    "            # LULC categorical band\n",
    "            a_lulc, _ = rio_mask.mask(\n",
    "                ds, [geom.__geo_interface__],\n",
    "                crop=True, filled=False, indexes=bm[\"lulc\"]\n",
    "            )\n",
    "            a_lulc = a_lulc[0] if a_lulc.ndim == 3 else a_lulc\n",
    "\n",
    "            # Probability bands required for m_informal\n",
    "            if (bm[\"atomistic\"] is None) or (bm[\"informal_subdivision\"] is None):\n",
    "                return out\n",
    "\n",
    "            a_atom, _ = rio_mask.mask(\n",
    "                ds, [geom.__geo_interface__],\n",
    "                crop=True, filled=False, indexes=bm[\"atomistic\"]\n",
    "            )\n",
    "            a_info, _ = rio_mask.mask(\n",
    "                ds, [geom.__geo_interface__],\n",
    "                crop=True, filled=False, indexes=bm[\"informal_subdivision\"]\n",
    "            )\n",
    "            a_atom = a_atom[0] if a_atom.ndim == 3 else a_atom\n",
    "            a_info = a_info[0] if a_info.ndim == 3 else a_info\n",
    "\n",
    "            # Valid pixels: where none of the bands are masked\n",
    "            if hasattr(a_lulc, \"mask\"):\n",
    "                valid = ~a_lulc.mask\n",
    "                if hasattr(a_atom, \"mask\"):\n",
    "                    valid &= ~a_atom.mask\n",
    "                if hasattr(a_info, \"mask\"):\n",
    "                    valid &= ~a_info.mask\n",
    "            else:\n",
    "                valid = np.ones_like(a_lulc, dtype=bool)\n",
    "\n",
    "            valid_px = int(valid.sum())\n",
    "            if valid_px < MIN_VALID_PX:\n",
    "                return out\n",
    "\n",
    "            # p_informal from lulc ∈ {2,3}\n",
    "            lulc_vals = a_lulc.data[valid] if hasattr(a_lulc, \"data\") else a_lulc[valid]\n",
    "            p_informal = float(\n",
    "                np.count_nonzero((lulc_vals == 2) | (lulc_vals == 3))\n",
    "            ) / valid_px\n",
    "\n",
    "            # m_informal: mean of atomistic + informal_subdivision (0–100), scaled to 0–1\n",
    "            atom = (a_atom.data[valid] if hasattr(a_atom, \"data\") else a_atom[valid]).astype(float)\n",
    "            info = (a_info.data[valid] if hasattr(a_info, \"data\") else a_info[valid]).astype(float)\n",
    "            m_informal = float(((atom + info) / PROB_DIVISOR / 2.0).mean())\n",
    "\n",
    "            out.update(\n",
    "                {\"valid_px\": valid_px, \"p_informal\": p_informal, \"m_informal\": m_informal}\n",
    "            )\n",
    "            return out\n",
    "    except ValueError:\n",
    "        # No overlap or geometry issues\n",
    "        return out\n",
    "\n",
    "\n",
    "def metrics(y_true, y_pred):\n",
    "    \"\"\"Basic classification metrics including balanced accuracy.\"\"\"\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    y_pred = np.asarray(y_pred).astype(int)\n",
    "    TP = int(((y_true == 1) & (y_pred == 1)).sum())\n",
    "    TN = int(((y_true == 0) & (y_pred == 0)).sum())\n",
    "    FP = int(((y_true == 0) & (y_pred == 1)).sum())\n",
    "    FN = int(((y_true == 1) & (y_pred == 0)).sum())\n",
    "    prec = TP / (TP + FP) if (TP + FP) else 0.0\n",
    "    rec  = TP / (TP + FN) if (TP + FN) else 0.0\n",
    "    f1   = 2 * prec * rec / (prec + rec) if (prec + rec) else 0.0\n",
    "    bal  = 0.5 * (\n",
    "        (TP / (TP + FN) if (TP + FN) else 0.0) +\n",
    "        (TN / (TN + FP) if (TN + FP) else 0.0)\n",
    "    )\n",
    "    return dict(TP=TP, FP=FP, TN=TN, FN=FN,\n",
    "                precision=prec, recall=rec, F1=f1, balanced_acc=bal)\n",
    "\n",
    "\n",
    "def sweep(df, col, taus, rule_name):\n",
    "    \"\"\"Apply thresholds over a probability column and compute metrics.\"\"\"\n",
    "    rows = []\n",
    "    for tau in taus:\n",
    "        yhat = (df[col] >= tau).astype(int)\n",
    "        m = metrics(df[\"rf_label\"], yhat)\n",
    "        m.update({\n",
    "            \"Rule / Comparison\": rule_name,\n",
    "            \"τ (threshold)\": tau,\n",
    "            \"n (blocks)\": len(df),\n",
    "        })\n",
    "        rows.append(m)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def pick_block_uid(df: gpd.GeoDataFrame) -> List[str]:\n",
    "    \"\"\"Return whichever ID columns exist for provenance.\"\"\"\n",
    "    return [c for c in POSSIBLE_ID_COLS if c in df.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416fcb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# 3️⃣ PER-COUNTRY WORKER\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "def process_country(country: str) -> dict:\n",
    "    \"\"\"\n",
    "    Process one country:\n",
    "      - Sample WRI metrics per block across all rasters.\n",
    "      - Write per-block CSVs, sweeps, overall/per-city counts.\n",
    "      - Return a small status dict.\n",
    "    \"\"\"\n",
    "    wri_dir = WRI_ROOT / country\n",
    "    gpkg    = GPKG_ROOT / f\"{country}_rf_preds.gpkg\"\n",
    "    out_dir = OUT_PARENT / country\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if not gpkg.exists():\n",
    "        return {\"country\": country, \"status\": \"no_gpkg\"}\n",
    "\n",
    "    tifs = list_wri_tifs(wri_dir)\n",
    "    if not tifs:\n",
    "        return {\"country\": country, \"status\": \"no_wri_tifs\"}\n",
    "\n",
    "    # Load blocks once (native CRS)\n",
    "    try:\n",
    "        blocks = read_blocks(gpkg, CITY_COL, RF_COL)\n",
    "    except Exception as e:\n",
    "        return {\"country\": country, \"status\": f\"gpkg_error: {e}\"}\n",
    "\n",
    "    id_cols = pick_block_uid(blocks)\n",
    "    rows, audit = [], []\n",
    "\n",
    "    for tif in tifs:\n",
    "        meta = read_raster_meta(tif)\n",
    "        bm = band_map_from_descriptions(meta[\"descriptions\"])\n",
    "\n",
    "        # BBox in blocks CRS for quick filtering\n",
    "        if blocks.crs != meta[\"crs\"]:\n",
    "            rb_blocks_crs = (\n",
    "                gpd.GeoDataFrame(geometry=[meta[\"bounds\"]], crs=meta[\"crs\"])\n",
    "                .to_crs(blocks.crs)\n",
    "                .geometry.iloc[0]\n",
    "            )\n",
    "        else:\n",
    "            rb_blocks_crs = meta[\"bounds\"]\n",
    "\n",
    "        # Spatial index if available\n",
    "        try:\n",
    "            subset_idx = blocks.sindex.query(rb_blocks_crs, predicate=\"intersects\")\n",
    "            subset = blocks.iloc[subset_idx].copy()\n",
    "        except Exception:\n",
    "            subset = blocks[blocks.intersects(rb_blocks_crs)].copy()\n",
    "\n",
    "        subset = subset[subset.intersects(rb_blocks_crs)]\n",
    "        if subset.empty:\n",
    "            audit.append({\"tif\": tif.name, \"overlapping_blocks\": 0})\n",
    "            continue\n",
    "\n",
    "        # Match CRS of raster\n",
    "        if subset.crs != meta[\"crs\"]:\n",
    "            subset = subset.to_crs(meta[\"crs\"])\n",
    "\n",
    "        # Per-block metrics on this TIFF\n",
    "        for _, r in subset.iterrows():\n",
    "            st = block_stats_for_tif(r, tif, bm)\n",
    "            rec = {\n",
    "                \"tif_name\": tif.name,\n",
    "                \"valid_px\": st[\"valid_px\"],\n",
    "                \"p_informal\": st[\"p_informal\"],\n",
    "                \"m_informal\": st[\"m_informal\"],\n",
    "                CITY_COL: r[CITY_COL],\n",
    "                \"rf_label\": int(r[RF_COL]) if pd.notna(r[RF_COL]) else np.nan,\n",
    "            }\n",
    "            for c in id_cols:\n",
    "                rec[c] = r[c]\n",
    "            rows.append(rec)\n",
    "\n",
    "        audit.append({\"tif\": tif.name, \"overlapping_blocks\": int(len(subset))})\n",
    "\n",
    "    # Save per-block + audit\n",
    "    stats_df = pd.DataFrame(rows)\n",
    "    audit_df = pd.DataFrame(audit).sort_values(\"tif\")\n",
    "\n",
    "    stats_csv = out_dir / f\"{country}_wri_informal_per_block.csv\"\n",
    "    audit_csv = out_dir / f\"{country}_wri_overlap_audit.csv\"\n",
    "    stats_df.to_csv(stats_csv, index=False)\n",
    "    audit_df.to_csv(audit_csv, index=False)\n",
    "\n",
    "    # Prepare comparison table (only blocks with RF labels + valid pixels)\n",
    "    cmp = stats_df.dropna(subset=[\"rf_label\"]).copy()\n",
    "    cmp = cmp[cmp[\"valid_px\"].fillna(0) >= MIN_VALID_PX].copy()\n",
    "    if cmp.empty:\n",
    "        return {\"country\": country, \"status\": \"no_overlap\"}\n",
    "\n",
    "    cmp[\"rf_label\"] = cmp[\"rf_label\"].astype(int)\n",
    "\n",
    "    # Threshold sweeps: p_informal and m_informal\n",
    "    country_tbl = pd.concat(\n",
    "        [\n",
    "            sweep(cmp, \"p_informal\", TAUS, \"WRI (p_informal)\"),\n",
    "            sweep(cmp, \"m_informal\", TAUS, \"WRI (m_informal)\"),\n",
    "        ],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "    sweep_csv = out_dir / f\"{country}_wri_vs_rf_threshold_sweep_country.csv\"\n",
    "    country_tbl.to_csv(sweep_csv, index=False)\n",
    "\n",
    "    # Per-block predictions for each τ\n",
    "    for tau in TAUS:\n",
    "        cmp[f\"pred_p_{tau:.2f}\"] = (cmp[\"p_informal\"] >= tau).astype(int)\n",
    "        cmp[f\"pred_m_{tau:.2f}\"] = (cmp[\"m_informal\"] >= tau).astype(int)\n",
    "\n",
    "    per_block_preds_csv = out_dir / f\"{country}_wri_vs_rf_per_block_with_preds.csv\"\n",
    "    cmp.to_csv(per_block_preds_csv, index=False)\n",
    "\n",
    "    # Overall counts (country-level)\n",
    "    n_blocks = len(cmp)\n",
    "    n_rf_dep = int((cmp[\"rf_label\"] == 1).sum())\n",
    "    n_rf_non = n_blocks - n_rf_dep\n",
    "\n",
    "    overall_rows = []\n",
    "    for tau in TAUS:\n",
    "        overall_rows.append({\n",
    "            \"country\": country,\n",
    "            \"τ (threshold)\": tau,\n",
    "            \"n_blocks\": n_blocks,\n",
    "            \"rf_deprived_n\": n_rf_dep,\n",
    "            \"rf_non_deprived_n\": n_rf_non,\n",
    "            \"WRI(p)_deprived_n\": int((cmp[f\"pred_p_{tau:.2f}\"] == 1).sum()),\n",
    "            \"WRI(m)_deprived_n\": int((cmp[f\"pred_m_{tau:.2f}\"] == 1).sum()),\n",
    "        })\n",
    "    overall_counts = pd.DataFrame(overall_rows)\n",
    "    overall_counts_csv = out_dir / f\"{country}_overall_deprived_counts_by_tau.csv\"\n",
    "    overall_counts.to_csv(overall_counts_csv, index=False)\n",
    "\n",
    "    # Per-city counts\n",
    "    per_city_tables = []\n",
    "    for tau in TAUS:\n",
    "        grp = (\n",
    "            cmp.groupby(CITY_COL)\n",
    "               .agg(\n",
    "                   n_blocks=(\"rf_label\", \"size\"),\n",
    "                   rf_deprived_n=(\"rf_label\", lambda s: int((s == 1).sum())),\n",
    "                   WRI_p_deprived_n=(f\"pred_p_{tau:.2f}\", \"sum\"),\n",
    "                   WRI_m_deprived_n=(f\"pred_m_{tau:.2f}\", \"sum\"),\n",
    "               )\n",
    "               .reset_index()\n",
    "        )\n",
    "        grp.insert(0, \"country\", country)\n",
    "        grp.insert(1, \"τ (threshold)\", tau)\n",
    "        per_city_tables.append(grp)\n",
    "\n",
    "    per_city_counts = pd.concat(per_city_tables, ignore_index=True)\n",
    "    per_city_counts_csv = out_dir / f\"{country}_per_city_deprived_counts_by_tau.csv\"\n",
    "    per_city_counts.to_csv(per_city_counts_csv, index=False)\n",
    "\n",
    "    return {\"country\": country, \"status\": \"ok\", \"n_blocks\": n_blocks, \"rf_dep\": n_rf_dep}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28778d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# 4️⃣ RUN IN PARALLEL ACROSS COUNTRIES + SAVE REPORT\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "countries = list_wri_countries(WRI_ROOT)\n",
    "print(\n",
    "    f\"Processing {len(countries)} country folder(s): \"\n",
    "    f\"{countries[:6]}{' ...' if len(countries) > 6 else ''}\"\n",
    ")\n",
    "\n",
    "N_JOBS = -1  # use all cores; set to e.g. 6 to limit\n",
    "results = Parallel(\n",
    "    n_jobs=N_JOBS, backend=\"loky\", prefer=\"processes\"\n",
    ")(\n",
    "    delayed(process_country)(c) for c in countries\n",
    ")\n",
    "\n",
    "report_df = pd.DataFrame(results)\n",
    "print(\"\\nStatus summary:\")\n",
    "print(report_df[\"status\"].value_counts())\n",
    "\n",
    "# Save high-level report alongside OUT_PARENT\n",
    "report_csv = OUT_PARENT.parent / \"wri_country_processing_report.csv\"\n",
    "report_df.to_csv(report_csv, index=False)\n",
    "print(f\"\\n✅ Country processing report saved to: {report_csv}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
