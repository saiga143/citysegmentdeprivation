{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d094ca4",
   "metadata": {},
   "source": [
    "# Analyse Random Forest Outputs (Thresholds, Curves, Confusion Matrices)\n",
    "\n",
    "This notebook:\n",
    "\n",
    "1. Reconstructs the training and test split using the saved holdout indices.  \n",
    "2. Loads the trained Random Forest model and computes predicted probabilities.  \n",
    "3. Explores classification thresholds (precision/recall/F1 vs threshold).  \n",
    "4. Plots PR and ROC curves on the holdout set.  \n",
    "5. Computes confusion matrices and detailed metrics at a chosen threshold (e.g., τ = 0.40).  \n",
    "6. Computes per-region metrics and balanced (downsampled) evaluations.  \n",
    "7. Computes Matthews correlation coefficient (MCC).\n",
    "\n",
    "**Inputs:**\n",
    "\n",
    "- Labeled RF training data (8 IDEABench cities):  \n",
    "  `../.../1_preprocessing/LabelledData_For_RF/*.csv`\n",
    "- RF training outputs:\n",
    "  - `region_mapping.json`\n",
    "  - `tables/file_list.csv`\n",
    "  - `tables/holdout_indices.csv`\n",
    "  - `rf_best_model.joblib`  \n",
    "  located in: `../01_training/rf_outputs/`\n",
    "\n",
    "**Outputs:**\n",
    "\n",
    "- Plots and tables under:  \n",
    "  `analysis_outputs/plots/`  \n",
    "  `analysis_outputs/tables/`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9c6836",
   "metadata": {},
   "source": [
    "# Imports and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c914a3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_curve, average_precision_score, roc_curve, roc_auc_score,\n",
    "    precision_score, recall_score, f1_score, confusion_matrix, accuracy_score,\n",
    "    classification_report\n",
    ")\n",
    "from sklearn.utils import check_random_state\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fd64f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid oversubscription of threads on some systems\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5625148e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder with per-city labeled training CSVs (from preprocessing step)\n",
    "CSV_FOLDER = Path(\"../../1_preprocessing/LabelledData_For_RF\")\n",
    "\n",
    "# RF training outputs (model, region mapping, file_list, holdout indices)\n",
    "RF_OUT_DIR = Path(\"../01_training/rf_outputs\")\n",
    "\n",
    "# Where this notebook will save analysis outputs\n",
    "ANALYSIS_OUT = Path(\"analysis_outputs\")\n",
    "PLOTS  = ANALYSIS_OUT / \"plots\"\n",
    "TABLES = ANALYSIS_OUT / \"tables\"\n",
    "\n",
    "PLOTS.mkdir(parents=True, exist_ok=True)\n",
    "TABLES.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"CSV_FOLDER: \", CSV_FOLDER.resolve())\n",
    "print(\"RF_OUT_DIR:\", RF_OUT_DIR.resolve())\n",
    "print(\"ANALYSIS_OUT:\", ANALYSIS_OUT.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c31d817",
   "metadata": {},
   "source": [
    "# Region mapping and file order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b27c029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load region mapping created during training\n",
    "REGION_MAP_PATH = RF_OUT_DIR / \"region_mapping.json\"\n",
    "with REGION_MAP_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    REGION_MAP = json.load(f)\n",
    "\n",
    "INVERSE_REGION = {v: k for k, v in REGION_MAP.items()}\n",
    "\n",
    "# Load exact file order used for training\n",
    "file_list_path = RF_OUT_DIR / \"tables\" / \"file_list.csv\"\n",
    "file_list = pd.read_csv(file_list_path)[\"file\"].tolist()\n",
    "print(f\"Files to load (ordered): {len(file_list)}\")\n",
    "file_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc3bd6d",
   "metadata": {},
   "source": [
    "# Build full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a27f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = \"slum_label1\"\n",
    "REGION_COL = \"REG1_GHSL\"\n",
    "\n",
    "predictor_cols = [\n",
    "    \"i5_par_area\", \"i1_pop_area\", \"i6_paru_area\", \"i8_paru_par\", \"B_AVG_SEG\",\n",
    "    \"i9_roads_par\", \"PARU_A_SEG\", \"B_AREA_SEG\", \"B_CV_SEG\",\n",
    "    \"REGION_CODE\",  # numeric code mapped from REG1_GHSL\n",
    "]\n",
    "\n",
    "def map_region(val):\n",
    "    if pd.isna(val):\n",
    "        return REGION_MAP[\"Unknown\"]\n",
    "    return REGION_MAP.get(str(val), REGION_MAP[\"Unknown\"])\n",
    "\n",
    "dfs = []\n",
    "for fname in file_list:\n",
    "    csv_path = CSV_FOLDER / fname\n",
    "    if not csv_path.exists():\n",
    "        print(f\"[WARN] Missing labeled CSV: {csv_path}\")\n",
    "        continue\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Add REGION_CODE\n",
    "    df[\"REGION_CODE\"] = df[REGION_COL].map(map_region)\n",
    "\n",
    "    keep = [c for c in predictor_cols if c != \"REGION_CODE\"] + [target_col, \"REGION_CODE\"]\n",
    "    missing = [c for c in keep if c not in df.columns]\n",
    "    if missing:\n",
    "        print(f\"[WARN] Skipping {fname} due to missing cols: {missing}\")\n",
    "        continue\n",
    "\n",
    "    df = df[keep]\n",
    "    dfs.append(df)\n",
    "\n",
    "if not dfs:\n",
    "    raise RuntimeError(\"No valid files after schema check.\")\n",
    "\n",
    "full_data = pd.concat(dfs, ignore_index=True)\n",
    "clean_data = full_data.dropna(subset=predictor_cols + [target_col]).reset_index(drop=True)\n",
    "\n",
    "X_full = clean_data[predictor_cols].to_numpy(dtype=float)\n",
    "y_full = clean_data[target_col].to_numpy()\n",
    "\n",
    "print(\"Data shape after cleaning:\", clean_data.shape)\n",
    "clean_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf4a151",
   "metadata": {},
   "source": [
    "# Rebuild train/test split from saved indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d7d70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_idx_df = pd.read_csv(RF_OUT_DIR / \"tables\" / \"holdout_indices.csv\")\n",
    "\n",
    "n = len(clean_data)\n",
    "test_mask = np.zeros(n, dtype=bool)\n",
    "test_indices = holdout_idx_df.loc[holdout_idx_df[\"is_test\"] == 1, \"global_index\"].to_numpy()\n",
    "test_mask[test_indices] = True\n",
    "train_mask = ~test_mask\n",
    "\n",
    "print(\"Train/Test sizes:\", train_mask.sum(), test_mask.sum())\n",
    "\n",
    "X_train, y_train = X_full[train_mask], y_full[train_mask]\n",
    "X_test,  y_test  = X_full[test_mask],  y_full[test_mask]\n",
    "\n",
    "region_test  = clean_data.loc[test_mask, \"REGION_CODE\"].to_numpy()\n",
    "region_train = clean_data.loc[train_mask, \"REGION_CODE\"].to_numpy()\n",
    "\n",
    "pd.Series(region_test).map(INVERSE_REGION).value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724eee3d",
   "metadata": {},
   "source": [
    "# Load trained model and inspect parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1917c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = RF_OUT_DIR / \"rf_best_model.joblib\"\n",
    "best = joblib.load(model_path)\n",
    "\n",
    "best_params_json = RF_OUT_DIR / \"tables\" / \"best_params.json\"\n",
    "if best_params_json.exists():\n",
    "    with best_params_json.open(\"r\") as f:\n",
    "        print(\"Best params from training:\", json.load(f))\n",
    "\n",
    "print(\"Model n_estimators (refit):\", getattr(best, \"n_estimators\", None))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afa5c23",
   "metadata": {},
   "source": [
    "# Score holdout and threshold sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38476067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted probabilities on holdout\n",
    "y_score = best.predict_proba(X_test)[:, 1]\n",
    "\n",
    "grid = np.linspace(0, 1, 101)\n",
    "rows = []\n",
    "for thr in grid:\n",
    "    y_pred = (y_score >= thr).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "    rec  = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1   = f1_score(y_test, y_pred, zero_division=0)\n",
    "    spec = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "    bal_acc = 0.5 * (rec + spec)\n",
    "    rows.append([thr, prec, rec, f1, spec, bal_acc, tn, fp, fn, tp])\n",
    "\n",
    "thr_df = pd.DataFrame(\n",
    "    rows,\n",
    "    columns=[\"threshold\",\"precision\",\"recall\",\"f1\",\n",
    "             \"specificity\",\"balanced_acc\",\"tn\",\"fp\",\"fn\",\"tp\"]\n",
    ")\n",
    "thr_df.to_csv(TABLES / \"threshold_sweep.csv\", index=False)\n",
    "\n",
    "best_row = thr_df.iloc[thr_df[\"f1\"].values.argmax()]\n",
    "best_thr = float(best_row[\"threshold\"])\n",
    "best_row.to_frame().T.to_csv(TABLES / \"best_threshold_f1.csv\", index=False)\n",
    "\n",
    "print(\"Threshold maximizing F1:\", best_thr)\n",
    "best_row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f786cfd6",
   "metadata": {},
   "source": [
    "# Plot threshold curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2abcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(thr_df[\"threshold\"], thr_df[\"precision\"], label=\"Precision\")\n",
    "plt.plot(thr_df[\"threshold\"], thr_df[\"recall\"],    label=\"Recall\")\n",
    "plt.plot(thr_df[\"threshold\"], thr_df[\"f1\"],        label=\"F1\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Threshold Sweep (Holdout)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOTS / \"threshold_sweep.png\", dpi=150)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3283324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PR curve\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_score)\n",
    "pr_auc = average_precision_score(y_test, y_score)\n",
    "pd.DataFrame({\"recall\": rec, \"precision\": prec}).to_csv(\n",
    "    TABLES / \"pr_curve_points.csv\", index=False\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(rec, prec)\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(f\"Precision–Recall (AP = {pr_auc:.3f})\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOTS / \"pr_curve.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "auc = roc_auc_score(y_test, y_score)\n",
    "pd.DataFrame({\"fpr\": fpr, \"tpr\": tpr}).to_csv(\n",
    "    TABLES / \"roc_curve_points.csv\", index=False\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(fpr, tpr, label=f\"AUC={auc:.3f}\")\n",
    "plt.plot([0,1],[0,1],'--',alpha=0.4)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC (Holdout)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOTS / \"roc_curve.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(f\"AUC={auc:.4f}, PR-AUC={pr_auc:.4f} at threshold maximizing F1 = {best_thr:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6499f1",
   "metadata": {},
   "source": [
    "# Helper: metrics at threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45314c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_at_threshold(y_true, y_proba, thr: float):\n",
    "    y_pred = (y_proba >= thr).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "    rec  = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1   = f1_score(y_true, y_pred, zero_division=0)\n",
    "    acc  = accuracy_score(y_true, y_pred)\n",
    "    spec = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "    bal_acc = 0.5 * (rec + spec)\n",
    "    return {\n",
    "        \"threshold\": thr,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"f1\": f1,\n",
    "        \"accuracy\": acc,\n",
    "        \"specificity\": spec,\n",
    "        \"balanced_acc\": bal_acc,\n",
    "        \"tn\": tn, \"fp\": fp, \"fn\": fn, \"tp\": tp\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e6f8b9",
   "metadata": {},
   "source": [
    "# Helper: confusion matrix plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecb5071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_proba, thr: float, savepath=None):\n",
    "    y_pred = (y_proba >= thr).astype(int)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    report = classification_report(y_true, y_pred, digits=3)\n",
    "    print(f\"--- Classification Report @ threshold = {thr:.2f} ---\")\n",
    "    print(report)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=\"Blues\")\n",
    "    fig.colorbar(im, ax=ax)\n",
    "\n",
    "    ax.set(\n",
    "        xticks=np.arange(2),\n",
    "        yticks=np.arange(2),\n",
    "        xticklabels=[\"Pred 0\", \"Pred 1\"],\n",
    "        yticklabels=[\"Actual 0\", \"Actual 1\"],\n",
    "        xlabel=\"Predicted\",\n",
    "        ylabel=\"Actual\",\n",
    "        title=f\"Confusion Matrix @ threshold = {thr:.2f}\"\n",
    "    )\n",
    "\n",
    "    vmax = cm.max()\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            val = cm[i, j]\n",
    "            color = \"white\" if cm[i, j] > vmax / 2 else \"black\"\n",
    "            ax.text(j, i, f\"{val}\", ha=\"center\", va=\"center\",\n",
    "                    color=color, fontsize=12, fontweight=\"bold\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if savepath:\n",
    "        plt.savefig(savepath, dpi=300)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43a13ac",
   "metadata": {},
   "source": [
    "# Helper: curves with threshold marked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a166df94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves_with_threshold(y_true, y_proba, thr: float, save_prefix: str):\n",
    "    # PR curve with marked threshold\n",
    "    prec, rec, thr_pr = precision_recall_curve(y_true, y_proba)\n",
    "    pr_auc = average_precision_score(y_true, y_proba)\n",
    "\n",
    "    idx_pr = np.argmin(np.abs(thr_pr - thr)) if len(thr_pr) > 0 else None\n",
    "    rec_thr  = rec[idx_pr] if idx_pr is not None else None\n",
    "    prec_thr = prec[idx_pr] if idx_pr is not None else None\n",
    "\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.plot(rec, prec, label=f\"AP={pr_auc:.3f}\")\n",
    "    if idx_pr is not None:\n",
    "        plt.scatter([rec_thr], [prec_thr])\n",
    "    plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Precision–Recall (Holdout)\")\n",
    "    plt.legend(); plt.tight_layout()\n",
    "    plt.savefig(PLOTS / f\"{save_prefix}_pr_curve_marked.png\", dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "    # ROC with threshold point\n",
    "    fpr, tpr, thr_roc = roc_curve(y_true, y_proba)\n",
    "    auc = roc_auc_score(y_true, y_proba)\n",
    "\n",
    "    idx_roc = np.argmin(np.abs(thr_roc - thr)) if len(thr_roc) > 0 else None\n",
    "    tpr_thr = tpr[idx_roc] if idx_roc is not None and idx_roc < len(tpr) else None\n",
    "    fpr_thr = fpr[idx_roc] if idx_roc is not None and idx_roc < len(fpr) else None\n",
    "\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.plot(fpr, tpr, label=f\"AUC={auc:.3f}\")\n",
    "    plt.plot([0,1],[0,1],'--',alpha=0.4)\n",
    "    if idx_roc is not None:\n",
    "        plt.scatter([fpr_thr], [tpr_thr])\n",
    "    plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC (Holdout)\")\n",
    "    plt.legend(); plt.tight_layout()\n",
    "    plt.savefig(PLOTS / f\"{save_prefix}_roc_curve_marked.png\", dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "    # Threshold sweep with vertical line\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(thr_df[\"threshold\"], thr_df[\"precision\"], label=\"Precision\")\n",
    "    plt.plot(thr_df[\"threshold\"], thr_df[\"recall\"],    label=\"Recall\")\n",
    "    plt.plot(thr_df[\"threshold\"], thr_df[\"f1\"],        label=\"F1\")\n",
    "    plt.axvline(thr, ls=\"--\", alpha=0.7)\n",
    "    plt.xlabel(\"Threshold\"); plt.ylabel(\"Score\")\n",
    "    plt.title(\"Threshold Sweep (Holdout)\")\n",
    "    plt.legend(); plt.tight_layout()\n",
    "    plt.savefig(PLOTS / f\"{save_prefix}_threshold_sweep_marked.png\", dpi=150)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d7b2df",
   "metadata": {},
   "source": [
    "# Choose operating threshold and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85750304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final operating threshold used in manuscript (can be changed)\n",
    "THR = 0.40  \n",
    "\n",
    "m = metrics_at_threshold(y_test, y_score, THR)\n",
    "pd.DataFrame([m]).to_csv(TABLES / f\"metrics_at_thr_{THR:.2f}.csv\", index=False)\n",
    "pd.DataFrame([m])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312d7b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(\n",
    "    y_test, y_score,\n",
    "    THR,\n",
    "    savepath=PLOTS / f\"confusion_matrix_thr_{THR:.2f}.png\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b9d2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves_with_threshold(\n",
    "    y_test, y_score,\n",
    "    THR,\n",
    "    save_prefix=f\"thr_{THR:.2f}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c436aad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for code in sorted(np.unique(region_test)):\n",
    "    mask = (region_test == code)\n",
    "    if mask.sum() == 0:\n",
    "        continue\n",
    "    mm = metrics_at_threshold(y_test[mask], y_score[mask], THR)\n",
    "    mm[\"region_code\"] = int(code)\n",
    "    mm[\"region_name\"] = INVERSE_REGION.get(int(code), \"Unknown\")\n",
    "    rows.append(mm)\n",
    "\n",
    "per_region_df = pd.DataFrame(rows)\n",
    "per_region_df.to_csv(\n",
    "    TABLES / f\"per_region_metrics_thr_{THR:.2f}.csv\", index=False\n",
    ")\n",
    "per_region_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31108d4c",
   "metadata": {},
   "source": [
    "# Balanced evaluation on test set (downsample negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd25c176",
   "metadata": {},
   "outputs": [],
   "source": [
    "BAL_N_REPEATS = 50\n",
    "BAL_RANDOM_SEED = 42\n",
    "\n",
    "def balanced_indices(y_true, rng):\n",
    "    \"\"\"Return indices for a balanced subset: all positives + sampled negatives of equal count.\"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    pos_idx = np.flatnonzero(y_true == 1)\n",
    "    neg_idx = np.flatnonzero(y_true == 0)\n",
    "    if len(pos_idx) == 0 or len(neg_idx) == 0:\n",
    "        raise ValueError(\"Cannot build balanced subset: one of the classes is missing in y_true.\")\n",
    "    neg_sample = rng.choice(neg_idx, size=len(pos_idx), replace=False)\n",
    "    sel = np.concatenate([pos_idx, neg_sample])\n",
    "    rng.shuffle(sel)\n",
    "    return sel\n",
    "\n",
    "rng = check_random_state(BAL_RANDOM_SEED)\n",
    "rows_bal = []\n",
    "cms = []\n",
    "\n",
    "for r in range(BAL_N_REPEATS):\n",
    "    sel = balanced_indices(y_test, rng)\n",
    "    m_bal = metrics_at_threshold(y_test[sel], y_score[sel], THR)\n",
    "    m_bal[\"repeat\"] = r\n",
    "    rows_bal.append(m_bal)\n",
    "\n",
    "    y_pred_bal = (y_score[sel] >= THR).astype(int)\n",
    "    cm_bal = confusion_matrix(y_test[sel], y_pred_bal)\n",
    "    cms.append(cm_bal)\n",
    "\n",
    "bal_df = pd.DataFrame(rows_bal)\n",
    "bal_df.to_csv(TABLES / f\"balanced_metrics_thr_{THR:.2f}.csv\", index=False)\n",
    "\n",
    "metric_cols = [\"precision\",\"recall\",\"f1\",\"accuracy\",\"specificity\",\n",
    "               \"balanced_acc\",\"tn\",\"fp\",\"fn\",\"tp\"]\n",
    "\n",
    "means = bal_df[metric_cols].mean()\n",
    "stds  = bal_df[metric_cols].std(ddof=1)\n",
    "\n",
    "bal_summary_row = pd.DataFrame([{\n",
    "    **{f\"{k}_mean\": float(means[k]) for k in metric_cols},\n",
    "    **{f\"{k}_std\":  float(stds[k])  for k in metric_cols},\n",
    "    \"n_repeats\": int(len(bal_df))\n",
    "}])\n",
    "\n",
    "bal_summary_row.to_csv(\n",
    "    TABLES / f\"balanced_metrics_summary_thr_{THR:.2f}.csv\", index=False\n",
    ")\n",
    "bal_summary_row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1941a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_avg = np.mean(np.stack(cms, axis=0), axis=0)\n",
    "cm_avg_rounded = np.rint(cm_avg).astype(int)\n",
    "pd.DataFrame(\n",
    "    cm_avg_rounded,\n",
    "    index=[\"True 0\",\"True 1\"],\n",
    "    columns=[\"Pred 0\",\"Pred 1\"]\n",
    ").to_csv(TABLES / f\"confusion_matrix_balanced_avg_thr_{THR:.2f}.csv\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "im = ax.imshow(cm_avg, interpolation='nearest', cmap=\"Blues\")\n",
    "fig.colorbar(im, ax=ax)\n",
    "ax.set(\n",
    "    xticks=np.arange(2), yticks=np.arange(2),\n",
    "    xticklabels=[\"Pred 0\",\"Pred 1\"], yticklabels=[\"Actual 0\",\"Actual 1\"],\n",
    "    xlabel=\"Predicted\", ylabel=\"Actual\",\n",
    "    title=f\"Averaged Confusion Matrix (Balanced, {BAL_N_REPEATS} draws) @ τ={THR:.2f}\"\n",
    ")\n",
    "vmax = cm_avg.max()\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        val = cm_avg[i, j]\n",
    "        color = \"white\" if val > vmax/2 else \"black\"\n",
    "        ax.text(j, i, f\"{val:.1f}\", ha=\"center\", va=\"center\",\n",
    "                color=color, fontsize=12, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOTS / f\"confusion_matrix_balanced_avg_thr_{THR:.2f}.png\", dpi=150)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b385b5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "mcc = matthews_corrcoef(y_test, (y_score >= THR).astype(int))\n",
    "print(\"Matthews correlation coefficient (MCC) at τ=%.2f: %.4f\" % (THR, mcc))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
